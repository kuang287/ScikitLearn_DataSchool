{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Evaluation Procedures\n",
    "\n",
    "**Training and testing on the same data**\n",
    "\n",
    "- Goal is to estimate likely performance of a model on out-of-sample data\n",
    "- But, maximizing training performance rewards overly complex models that won't necessarily generalize\n",
    "- Unnecessarily complex models overfit the training data:\n",
    "    - Will do well when tested using the in-sample data\n",
    "    - May do poorly on out-of-sample data\n",
    "    - Learns the \"noise\" in the data rather than the \"signal\"\n",
    "\n",
    "**Train/test split**\n",
    "\n",
    "- Split the dataset into two pieces, so that the model can be trained and tested on different data\n",
    "- Testing performance is a better estimate of out-of-sample performance (compared to training performance)\n",
    "- But, it provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing performance\n",
    "- Allows you to easily inspect your testing results (via confusion matrix or ROC curve)\n",
    "\n",
    "**K-fold cross-validation**\n",
    "\n",
    "- Systematically create \"K\" train/test splits and average the results together\n",
    "- Cross-validated performance is a more reliable estimate of out-of-sample performance (compared to testing performance)\n",
    "- Runs \"K\" times slower than train/test split\n",
    "\n",
    "## Comparing Evaluation Metrics for Classification Problems\n",
    "\n",
    "**Classification accuracy/error**\n",
    "\n",
    "- Classification accuracy is the percentage of correct predictions (higher is better)\n",
    "- Classification error is the percentage of incorrect predictions (lower is better)\n",
    "- Easiest classification metric to understand\n",
    "\n",
    "**Confusion matrix**\n",
    "\n",
    "- Confusion matrix gives you a better understanding of how your classifier is performing\n",
    "- Allows you to calculate sensitivity, specificity, and many other metrics that might match your business objective better than accuracy\n",
    "\n",
    "**ROC curves and Area Under the Curve (AUC)**\n",
    "\n",
    "- Allows you to visualize the performance of your classifier across all possible classification thresholds, thus helping you to choose a threshold that appropriately balances sensitivity and specificity\n",
    "- Still useful when there is high class imbalance (unlike classification accuracy/error)\n",
    "- Harder to use when there are more than two response classes\n",
    "\n",
    "**Log loss**\n",
    "\n",
    "- Most useful when well-calibrated predicted probabilities are important to your business objective\n",
    "\n",
    "## Comparing Evaluation Metrics for Regression Problems\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "- Mean of the absolute value of the errors\n",
    "- Easiest regression metric to understand\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "- Mean of the squared errors\n",
    "- More popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world\n",
    "\n",
    "**Root Mean Squared Error (RMSE)**\n",
    "\n",
    "- Square root of the mean of the squared errors\n",
    "- Even more popular than MSE, because RMSE is interpretable in the \"y\" units"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
